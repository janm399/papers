\input{../../common.tex}

\addbibresource{root.bib}

\title{Machine Learning to the rescue}

\author{Jan Macháček}

\newcommand{\joel}[1]{\\ \emph{Joel: #1}}

\begin{document}

\twocolumn[
  \begin{@twocolumnfalse}
    \maketitle
    \begin{abstract}
      Machine learning to the rescue!... the first question is ``to the rescue of what?''; immediately followed by ``when is it indeed rescued?''. The answers to these questions are crucial; luckily, the software engineering process is quite used to asking and answering these questions. Careful project analysis and inception, followed by continous integration and continous deployment in development (supported by adequate tests); all overseen by systematic project governance leads to successful software projects. This paper's proposition is that machine learning projects that are to apply established machine learning approaches and algorithms are no different than any other software project; and they must follow all practices of software engineering. 
    \end{abstract}
  \end{@twocolumnfalse}
]

\section{Software engineering}
...

The Joel test\cite{joeltest} is a product of one man's biased, ad-hoc, informal, ... view of what makes successful software projects. Annoyingly, high scores on the Joel test correlate with successfully delivered software projects. Spolsky acknowledges that it's possible for a small team of cowboys to deliver amazing software with score of 0, as much as it is possible for a team that scores 12 to be the software equivalent of the Titanic. Nevertheless, high scores on the test correlate with good practices and discipline, which usually leads to good software.

This paper's proposition is that ``business'' projects that use machine learning are no different than any other software project; and that all practices of software engineering have to be applied to the machine learning subsystems. Specifically, that the 12 points on the Joel test are just as applicable, but with additional 12 points. 

\begin{enumerate}
  \item Version control for the ML models and data sets used to train them
        \joel{Do you use source control?}
  \item Single-step / automated data selection, model training, evaluation, and deployment
        \joel{Can you make a build in one step?}
  \item [At least] daily training and deployment process
        \joel{Do you make daily builds?}
  \item The results of the BI queries that humans process define what ML should solve
        \joel{Do you have a bug database?}
  \item Versioned, testable; continuously tested and sanity-checked BI
        \joel{Do you fix bugs before writing new code?}
  \item BI that allows any query to be answered in under 10 minutes
        \joel{Do you have a spec?}
Do you have an up-to-date schedule?
Do programmers have quiet working conditions?
Do you use the best tools money can buy?
Do you have testers?
Do new candidates write code during their interview?
Do you do hallway usability testing?
  \item Versioned, testable; continuously tested and sanity-checked analytics (BI)  
  \item Monitoring on the BI environment to identify queries that use normalised data
  \item The results of the BI queries that humans process define what ML should solve
  \item Ingestion components decoupled from the rest of the system
  \item Versioned, testable; continuously tested and sanity-checked data sets 
  \label{txt:ml-code}\item Pre-computed ``return constant'' model
  \item Versioned, testable; continuously tested and sanity-checked model storage with training and validation data set references
  \item Model deployer and ``debugger''
\end{enumerate}

\begin{enumerate}
  \item Versioned, testable; continuously tested and sanity-checked analytics (BI)  
  \item Any BI query can be answered under 10 minutes
  \item Monitoring on the BI environment to identify queries that use normalised data
  \item The results of the BI queries that humans process define what ML should solve
  \item Ingestion components decoupled from the rest of the system
  \item Versioned, testable; continuously tested and sanity-checked data sets 
  \label{txt:ml-code}\item Pre-computed ``return constant'' model
  \item Versioned, testable; continuously tested and sanity-checked model storage with training and validation data set references
  \item Model deployer and ``debugger''
\end{enumerate}

But where is the ML that builds the model? That's the code that the engineering teams need to build to replace \autoref{txt:ml-code}. 

Once the first four steps are known, the engineering teams can implement the remaining steps of the pipeline. If the system that is to take advantage of ML is event-based, the event delivery mechanism provides the decoupling, resulting in architecture shown in \autoref{fig:pipeline-es}.

\fig{pipeline-es.png}{pipeline-es}{ML pipeline in event-based system}

If the front-end system is not event-sourced, the ingestion must be decoupled using a read-only replica of the live data. Notice in \autoref{fig:pipeline-nes} the flow of the data: the data is pushed into the read-only replica in the first step to allow the front-end system to control the load on its data store; from the read-only replica, the data is pulled into the ML data store. 

\fig{pipeline-nes.png}{pipeline-nes}{ML pipeline in non event-based system}

Regardless of the approach used (or even if a hybrid approach is deployed), the entire system has to be aware of any back-pressure.

The ML team maintains the tooling for the pipeline, consults on the best models, researches, ...; but the product teams (that ultimately work on the service that \emph{uses} the model) have the first dibs on implementing the model. Successful implementation of this strategy means that anyone can implement a new model (even if only to just see what will happen!), train it, debug it, and deploy it all within a single day. All the mechanics of data ingestion, storage, versioning; runtime of training a model, evaluation, storage, versioning; debugging and deploying; and the usage is all implemented. 

In this sense, the machine learning code is just like any other ordinary code; it is subject to all the high engineering standards and safeguards.

\section{Implementation choices}
Attach technologies to the blocks in \autoref{fig:pipeline-es} and \autoref{fig:pipeline-nes}. Show on practical example, attempt to pull out reusable blocks and show examples of good and bad code. Think the RAZCP ML experiments; demonstrating just how much time the initial research and experiments take. Reinventing the experimental wheel must be avoided--the ML team should curate the bootstrapping environment, making it available ``on demand'' for other teams. The work on the ML core code can only successfully happen when all other pieces are in place. 

\subsection{Initial research}
The practical application of the ``ML enablement process'' aimed to deliver failure predictions and automatic error recovery based in an event-based architecture system. The system already published seemingly too many events, but the detail in the events were very useful in building the data sets for the ML project. The first step the team needed to take was to build traditional business intelligence database--the goal was to find out whether our human knowledge and experience allowed us to find meaningful information in the data.

The system's journal (Apache Kafka\cite{kafka}) is configured with 7 days' message retention policy; at the very start of the work, the team was able to download events for the last 7 days. This was approximately \SI{60}{\gibi\byte} of uncompressed Protocol Buffers\cite{protobuf} binary messages. Because of the compactness of the binary wire representation of the Protocol Buffers messages, this translated to \SI{60}{\gibi\byte} of a RDBMS (MySQL\cite{mysql}) storage requirements. The loader program performed batch JDBC inserts into the MySQL tables matching the topic name.


\subsection{Buy vs. build (AWS services vs. custom code)}


\subsection{Testing}

\section{Compromises}
How to manage the growing data sizes and time to train the models; particularly with complex models that need a lot of computation to train? Where to keep the old data and old models? Is there ever value in digging out models and data that are 10 versions old?

\printbibliography

\end{document}
