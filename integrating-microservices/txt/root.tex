\documentclass[10 pt, twocolumn]{article}
\input{../../common.tex}

\addbibresource{root.bib}

\title{Integrating microservices}

\author{Jan Mach{\'a}\v{c}ek%$^{1}$% <-this % stops a space
\thanks{Supported by BAMTECH Media}% <-this % stops a space
\thanks{$^{1}$J. Machacek is the Senior Principal Engineer at BAMTECH Media, Houldsworth Mill, Houldsworth Street, Reddish, SK5 6DA, UK {\tt\small jan.machacek at bamtechmedia.com}}%
}

\begin{document}

\twocolumn[
  \begin{@twocolumnfalse}
    \maketitle
    \begin{abstract}
      When a system needs to spread across asynchronous and unreliable communications boundary, its components on either side of this boundary have to use precise API. The protocol

      Kafka is cool. Now, how to use it, run it, and what are the pitfalls.
    \end{abstract}
  \end{@twocolumnfalse}
]

\section{Stateful distributed systems}
Systems whose components run on multiple nodes are difficult to design without obvious errors, difficult to build, difficult to run, and difficult to maintain; the reasons that justify this complexity are resilience and performance. Resilience refers to the system's ability to continue to operate even if some of its components are failing; informally, performance refers to the system's ability to handle large incoming workload quickly. The precise definitions of ``large'' and ``quickly'' depend on the details of system being built. Availability is the ratio of the system's uptime over the time the system is able to serve clients. Intuitively, systems with high resilience and high ability to handle workloads result in high availability. 

\subsection{CAP}
The CAP theorem says that out of the three properties (consistency, availability, and partition tolerance), a system can satisfy only two at the same time; a \emph{distributed system} needs to include partition tolerance in order to be able to cope with the reality of running on nodes that might become disconnected through hardware or software failure. This leaves only one of consistency and availability. There are many levels of consistency and availability; these levels can be set differently for each of the system's components. Moreover, consistency and availability aren't binary: it is possibe to select a very loose consistency level, leaving lots of room for good availability.

\begin{example}[CAP intuition]
A good way to think about CAP is to consider a database that is to be consistent and partition-tolerant, and that spreads its data on multiple nodes. The command \pcode{truncate table X} can be considered to succeed if all nodes that make up the system can all agree that they have completed their part of the work. If just one node cannot confirm the completion (because it crashed, because it took too long to reply, ...), the system's only choice is to stop accepting further work. It is configured to be consistent, but it cannot verify this; it can switch to a ``recovery'' or ``maintenance'' mode until it becomes consistent again (automatically or by some kind of administrative intervention).
\end{example}

\subsection{Why?}
A system is spread over multiple nodes because it is too big for just one node, or because the risk of node failure taking down the entire system is unacceptable. The size of the benefit of distributing the system over mulitple noes depends on the system's workload and architecture. The best-case scenario is a system whose work units fit entirely on one node, and do not need to communicate with any other nodes. Consider a system that can perform independent image classification where the image to be classified fits into one input message, the classifier does not need any further input to perform the classification, and the result can be delivered in a single output message. Such a system is completely stateless and so very simple to distribute: the increase the system's throughput, one just needs to add nodes.
In order to benefit from the increased processing power that more nodes bring, there must be a way to distribute the workload to the worker nodes. This requires some form of load balancer that maintains a set of available nodes and routes requests to them, or a workload repository that the nodes can query to obtain the next work item. Both the load balancer or the workload repository need to be distributed systems themselves so that they don't become the single point of failure for the distributed system. The load balancer maintains the set of available worker nodes; the workload repository spreads the workload over its nodes: this introduces state and forces the configuration of the load balancer or the work repository to consider the CA balance.

It might need to store too much data to fit on the built-in disks, it might need to do too much computation for the CPU to handle, it might need to keep too much data to keep in RAM, etc. This reasoning assumes that all these local resources (CPU, RAM, disk) are 

\begin{figure}[h]
  \begin{tikzpicture}
    \begin{axis}[
      height=7cm, 
      width=9cm, 
      yminorgrids=true,
      xtick=data,
      ymode=log,
      xlabel={Program}, 
      ylabel={Relative execution time},
      ymin=1,ymax=1000,
      xmin=0,xmax=4,
      legend pos=north west,
      legend cell align={left},
      xticklabel style={rotate=90},
      xticklabels={Simple, Some ex., All ex., Enterprise, Bad async}
      ]

      \addplot+ coordinates {
        (0, 1)
        (1, 5.162790698)
        (2, 42.51162791)
        (3, 3)
        (4, 868.5)
      };
      \addlegendentry{Scala}

      \addplot+ coordinates {
        (0, 3.837209302)
        (1, 8.023255814)
        (2, 66)
        (3, 25.12790698)
        (4, 959.9883721)
      };
      \addlegendentry{C++}

      \addplot+ coordinates {
        (0, 1.860465116)
        (1, 2.255813953)
        (2, 4.11627907)
        (3, 2.244186047)
        (4, 44.30232558)
      };
      \addlegendentry{Go}

      \addplot+ coordinates {
        (0, 3.965116279)
        (1, 3.930232558)
        (2, 3.930232558)
        (3, 3.918604651)
        (4, 615.5116279)
      };
      \addlegendentry{Haskell}
    \end{axis}
  \end{tikzpicture}
  \caption{Simple computation}
  \label{plot:simple-computation}
\end{figure}

\begin{figure*}[h]
  \begin{tikzpicture}
    \draw[pink,   -,fill=pink   ] (0, 0)    rectangle (17, 1.35); \node[right] at (13.8, 0.7) {heart beat};
    \draw[blue!20,-,fill=blue!20] (0, 1.35) rectangle (17, 2.8);  \node[right] at (13.8, 2.0) {drink a cup of tea};
    \draw[pink,   -,fill=pink   ] (0, 2.8)  rectangle (17, 4.3);  \node[right] at (13.8, 3.6) {average commute};
    \draw[blue!20,-,fill=blue!20] (0, 4.3)  rectangle (17, 6);    \node[right] at (13.8, 5.1) {blockbuster film};
    \draw[pink,   -,fill=pink   ] (0, 6)    rectangle (17, 7.3);  \node[right] at (13.8, 6.7) {MAN $\rightarrow$ SFO};
    \draw[blue!20,-,fill=blue!20] (0, 7.3)  rectangle (17, 8.5);  \node[right] at (13.8, 7.7) {holiday};
    \draw[pink,   -,fill=pink   ] (0, 12)   rectangle (17, 13.4); \node[right] at (13.8, 12.7) {history of USA};

    \begin{axis}[
      height=15cm, 
      width=15cm, 
      yminorgrids=true,
      xtick=data,
      ymode=log,
      xlabel={Program}, 
      ylabel={Relative execution time},
      ymin=1,ymax=10516002326,
      xmin=0,xmax=9,
      legend pos=north west,
      legend cell align={left},
      xticklabel style={rotate=90},
      xticklabels={ i, ii, iii, iv, v, vi, vii, viii, ix, x}
      ]

      \addplot+ coordinates {
        (0, 1)
        (1, 5.162790698)
        (2, 42.51162791)
        (3, 3)
        (4, 868.5)
        (5, 8988.348837)
        (6, 10586.84884)
        (7, 60683.83721)
        (8, 525800.1163)
        (9, 10516002326)
      };
      \addlegendentry{Scala}

      \addplot+ coordinates {
        (0, 3.837209302)
        (1, 8.023255814)
        (2, 66)
        (3, 25.12790698)
        (4, 959.9883721)
        (5, 2610.965116)
        (6, 4225.232558)
        (7, 47250.87209)
        (8, 512367.1512)
        (9, 10247343023)
      };
      \addlegendentry{C++}

      \addplot+ coordinates {
        (0, 1.860465116)
        (1, 2.255813953)
        (2, 4.11627907)
        (3, 2.244186047)
        (4, 44.30232558)
        (5, 6135.988372)
        (6, 7553.034884)
        (7, 116279.0581)
        (8, 581395.3372)
        (9, 11627906744)
      };
      \addlegendentry{Go}

      \addplot+ coordinates {
        (0, 3.965116279)
        (1, 3.930232558)
        (2, 3.930232558)
        (3, 3.918604651)
        (4, 615.5116279)
        (5, 9842.953488)
        (6, 11480.0814)
        (7, 116279.0581)
        (8, 581395.3372)
        (9, 11627906744)
      };
      \addlegendentry{Haskell}
    \end{axis}
  \end{tikzpicture}
  \begin{enumerate}[i]
    \item Simple
    \item Some ex.
    \item All ex.
    \item Enterprise
    \item Bad async
    \item \SI{1}{\mebi\byte} from \pcode{/dev/zero}
    \item \SI{1}{\mebi\byte} from regular file
    \item REST API call with \SI{100}{\micro\second} latency
    \item REST API call with \SI{10}{\milli\second} latency
    \item REST API call with \numprint{0.00001} error rate + \SI{10}{\milli\second} latency
  \end{enumerate}
  \caption{Simple computation}
  \label{plot:simple-computation-xl}
\end{figure*}

\section{State in distributed system}
Imagine two systems that need to share information; a good starting point may be an endpoint where one system can request the state of the other system. If the first system needs to keep ``in sync'' with the second system, it needs to periodically query the endpoint to obtain the state from the second system. This keeps the two systems independent and decoupled, but it means that the systems might miss important state changes that happen between the two polls, and it makes both systems do needless work by querying the endpoints, even if the state remains the same.

This inefficiency, the complexity of building reliable and easy-to-use endpoints, the reasoning that both systems are views on the same underlying data, the possible time-pressures to deliver working integration, leads to the \emph{database as integration layer} anti-pattern\cite{eip}. Never mind the \emph{anti} in anti-pattern: what would happen if the systems actually decided to use a shared database? As the database spreads over multiple networked nodes, which introduces unreliability and delay, and means that the database administrators now need to choose the two of the C$\cdot$A$\cdot$P--as long as one of the choices is P. This leaves the DBAs with a decision between A and C. A database that keeps its state on multiple nodes may keep them ``in sync'' by sending the nodes messages that describe the state transitions to be applied to the state. The state that the database (in the sense of all its nodes) keeps is then simply a reliable playback of all the messages. The state is a snapshot of the stream of messages at a given point in time. However, the decision between C and A still remains. If the choice is C, then all nodes have to agree that they have indeed received and applied all messages up to some point in time; in case the messages aren't received, or if there is no agreement, the database must become unavailable. If the choice is A, then there is the risk that a node that is being queried has not yet received all update messages; querying another node may yield different result; repeating the same query on the original node later may yield different result.

Using a database as an integration layer does not make the difficult choices of a distributed system go away, yet it keeps all the disadvantages of the anti-pattern. It forces shared data model, which may couple two (multiple!) systems together in the same release cycle; it is also a recipe for slowly-growing monolith, where one of the systems gains more and more functionality (the one with the largest number of developers); the other systems become lighter--after all, they are just views on the same shared state. The shared data model also means that two (multiple!) systems have to agree on usable data models for each.

Using messaging 

\subsection{Notification}
Notification sent from one system to indicate that ``something's up'' are a useful way to reduce the busy polls. Think of the notifications as interrupts that only indicate that now is a good time to make a more detailed query. \autoref{fig:es-notification} shows the flow where system \circled{A} emits a notification that carries some simple playload. System \circled{B} consumes the notification, examines the payload and makes a request to query for more details.

\fig{es-notification.png}{es-notification}{Notification}

The contents of the notification sent from \circled{A} to \circled{B} is small: an identifier of the entity that has changed, a range of entities that have changed, and similar. It is the responsibility of \circled{B} to make the appropriate queries back to \circled{A}. 

\subsection{State transfer}

\subsection{Event-sourcing}

\subsection{CQRS/ES}

\begin{table}[h]
    \begin{tabular}{lcccc}
        \toprule
        & \rot{Reduces busy polls} & \rot{Maintains consistency} & \rot{Does not increase availability} & \rot{CQRS/ES} \\
        \midrule
        Notification     & x & x & x & x \\
        State transfer   & x & - & - & x \\
        Event-sourcing   & x & - & x & x \\
        CQRS/ES          & - & - & x & x \\
        \bottomrule
    \end{tabular}
    \caption{Bit damage}
    \label{tbl:bit-damage}
\end{table}


\section{Message-driven systems}
If the systems that are to be integrated are event-driven--each system publishes messages as it proceeds with its operation, and if each system accepts messages at any time without previously initiating a request--then the integration can take advantage of this transfer of information. Relying entirely on [persistent] messaging can be burdensome; it is reasonable to rely on snapshots to remove the need to replay the messages from the beginning of time at start-up. 

It might seem reasonable to rely on an endpoint that can deliver the entire snapshot in one response at start-up. Ignoring the potential problems with the size of the response, this will make the two systems to be integrated more coupled together. Specifically, it will remove the decoupling in time that the persistent messaging mechanism brings. 

Consider two systems that need to share parts of their state. If both systems are event-driven, the task is simpler: all that needs to be done is to deliver the messages from the first system to the second system. Even though it is possible for the second system to accept the ``internal'' messages from the first system, it is safer to add a component that implements translation between the two systems. (This translation component can be \pcode{identity}, but it should be there.) \autoref{fig:2s-es} illustrates the event-driven scenario.

\fig{2s-es.png}{2s-es}{Integrating two systems}

Both systems \circled{A} and \circled{B} have their private journal and snapshot stores. The sharing of state begins by initial query that pulls in the appropriate snapshot of state from system \circled{A} into \circled{B} in \circled{1}; then the system \circled{B} follows the state changes in \circled{A} by acting on the [translated] messages it receives from \circled{A}. The system \circled{A} is not aware of any connection; it is not even aware of the translation component; all that the system \circled{A} is concerned about is delivery of messages to its messaging boundary. The flow is illustrated in \circled{2}, \circled{3}, and \circled{4}.

\autoref{fig:2s-es} does not explain how the messages cross the boundary between the systems; it is clear that there needs to be some kind of connector that sits either on the \circled{A} or the \circled{B} side. It is better for the connector to be on the \circled{B} side. \circled{B} wants to receive messages from \circled{A}, so it should be responsible for making the arrangements: it should verify that it is indeed connecting to the \emph{right} \circled{A} (checking certificates, for example); even more importantly, the connector needs to \emph{consume} messages from \circled{A}, and then \emph{publish} the same messages to \circled{B}. Publishing is potentially much more complex procedure (what is the acceptable confirmation / acknowledgement, what are the maximum batch sizes, time-outs, etc.), and should be in full control of \circled{B}\cite{mirrormaker}.  

% If the two systems are not event-driven, but rather simpler CRUD-based services, it may be possible to pull the state update messages from the data stores that the systems use; either using the datastore's native functionality or by implementing a custom service. Ideally, the only change to the integration flow will be the source and destination of the messages: the source becomes the state database in \circled{A}; the destination becomes \circled{B}'s APIs (see \autoref{fig:2s-crud}). 

% \fig{2s-crud.png}{2s-crud}{Integrating CRUD systems}

All other approaches are recpies for problems (now or in the very near future). By far the worst-case scenario is database-to-database integration; particularly where \circled{A} writes directly to the database of \circled{B}. This is the most brittle and most dangerous approach; only slightly less dangerous and brittle solution is for \circled{B} to pull data from \circled{A}'s database. If the system \circled{B} is not event-driven, the only approach is to poll data from \circled{A}\footnote{To avoid the inefficiency of frequent polls with no changes, or too infrequent polls missing changes, add a component that can act as fancy circuit-breaker}. 

If the two systems are connected ``from the beginning of time'' (that is \circled{B} receives all messages from \circled{A} from the first message), or if the two systems only need to synchronise a fixed number of messages, there will be no need to worry about the initial snapshot poll from \circled{A}. In other cases, \circled{B} requests the initial state from \circled{A} and then subscribes to the updates. Unfortunately, \circled{A} keeps updating its state, even as it serves the response to the initial poll to \circled{B}. A simple algorithm that \circled{A} can follow--\pcode{state <- poll(B); subscribe(B);}--runs the risk of missing messages that happened just after the initial poll. A systematic way of solving this is to include an offset in the initial poll response specifying the position at which the snapshot was taken. \circled{B} is responsible for subscribing to updates from \circled{A} from that offset. This way, there is no risk in missing updates from \circled{A}. An alternative is to measure the risk of missing an update vs. doing too much work and subscribing from the end of the topic less some arbitrary offset.

A typical event-driven system with persistent messaging implemented in Kafka typically uses multiple topics, each with multiple partitions. The offset is associated with each topic partition; the response to the initial poll request needs to to include a map of \pcode{(topic, partition) -> offset}. \autoref{fig:2s-es-msgs} illustrates the message contents and flow.

\fig{2s-es-msgs.png}{2s-es-msgs}{Messages}

The first message is the response to the request for initial state. It contains the state (in the \pcode{body} field) and the topic partition offsets used to construct the state. After processing the initial state \circled{B} subscribes to the topics indicated by \circled{A} to receive updates. This gurarantees that no messages will be lost, though it does not guarantee a \emph{total ordering} of the update messages on the topic; \circled{B} has to be able to deal with out-of-order delivery of the update messages\footnote{Distributed messaging systems such as Kafka\cite{kafka} or AWS Kinesis\cite{kinesis} only have total order in a specific partition of a topic.}. \autoref{code:2s-es-msg-flow} shows a typical code in \circled{B}.


\begin{lstlisting}[caption={Message flow}, label={code:2s-es-msg-flow}, language=Pseudo, escapechar=|]
if not initialised then 
    (tps, state) <- poll(A)
    insert(state)
    subscribe(mode = SemiAutomatic, tpos = tpos, 
              onMessage = /\m -> update(m))
else
    subscribe(mode = FullyAutomatic,
              onMessage = /\m -> update(m))
end if
\end{lstlisting}

The \pcode{not initialised} indicates that \circled{B} has never been synchronised with A; the \pcode{mode = SemiAutomatic} instructs the messaging infrastructure to handle offset management, but allow initial offset specification; the \pcode{mode = FullyAutomatic} indicates that the messaging infrastructure should start the subscription from the last consumed offset.

\section{Implementation}
This paper will offer two implementation approaches: one that relies on Apache Kafka and one that uses the AWS tooling. Other implementations will find many common principles between the two.

\subsection{Kafka-based}
If the system cannot take advantage of any built-in functionality to the underlying infrastructure, it will be necessary to build the connectors and transformers ourselves. 

System 1 <=> MySQL
          -> Kafka => Mirror Maker -> Kafka Connector -> Kafka -> Kafka Connector -> MySQL <=> System 2

\subsection{AWS-based}

System 1 <=> Dynamo 
          -> Kinesis => Kinesis Replicator -> Lambda -> Kinesis -> Lambda -> MySQL <=> System 2

\section{Scalability and resilience}
TODO.

\section{Practical applicaiton}
X requested a feature in the Y system to implement a REST API endpoint that can be queried for the current state of a particular media assignment; but this request should also create a subscription for any changes to be delivered to the X system over a Kafka topic. The motivation for the feature was X’s need to know Y’s state, but to avoid doing periodic polling. (Polling, the team X reasoned, is just not cool, and might still miss an important change that happens between the two polls.)
\begin{itemize}
  \item Y should not implement state that is only useful for X. The state Y would hold is not needed for its operation, it only serves one particular client;
  \item Maintaining subscriptions requires more state to be maintained inside Y; this state needs to be recoverable in case of failures, adding significant complexity to Y;
  \item  Even with the subscription in place, the system (comprising Y and X) remains eventually consistent; adding arbitrary delays to the subscription does not solve this. (e.g. only start sending updates 40 seconds later to be sure that there was enough time for the state to become consistent does not solve the problem of eventual consistency; it simply allows us to pretend that the system is consistent!)
  \item There is no clear understanding (maybe there cannot be clear understanding) of when a subscription ends, or what identifies a client
\end{itemize}
Instead, I guided the teams to understand a better approach (bite the bullet and have X subscribe to the existing updates topic and maintain its own model, which duplicates information in principle, but allows most suitable representation of the information to exist in each system; it also maintains clear separation of responsibilities of each system; finally, it does not lock the two systems in the same release cycle).


\printbibliography

\end{document}